{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Complete Guide on DataFrame Operations in PySpark\n\nhttps://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n\n## Creating a SparkContext\n\nFirst we need to create a SparkContext. We will import this from pyspark:"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# from pyspark import SparkContext", 
            "outputs": [], 
            "execution_count": 1
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Now create the SparkContext,A SparkContext represents the connection to a Spark cluster, and can be used to create an RDD and broadcast variables on that cluster.\n\n*Note! You can only have one SparkContext at a time the way we are running things here.*"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# sc = SparkContext()", 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# Spark context alreday exists!\nsc", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "<pyspark.context.SparkContext at 0x7fdeb6bae450>"
                    }, 
                    "execution_count": 21
                }
            ], 
            "execution_count": 21
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "sc.version", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "u'1.6.0'"
                    }, 
                    "execution_count": 136
                }
            ], 
            "execution_count": 136
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Creating DataFrame from RDD"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.sql import Row\nl = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\nrdd = sc.parallelize(l)\npeople = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\nschemaPeople = sqlContext.createDataFrame(people)", 
            "outputs": [], 
            "execution_count": 137
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "type(schemaPeople)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "pyspark.sql.dataframe.DataFrame"
                    }, 
                    "execution_count": 138
                }
            ], 
            "execution_count": 138
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Creating the DataFrame from CSV file"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'cc29768790ec45439a43668592b02f84')\n    hconf.set(prefix + '.username', 'd47dac60c7684410842aa453908da4ca')\n    hconf.set(prefix + '.password', 'R1o7wzw?37&dHIMq')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name)\n\ntrain = sqlContext.read.format('com.databricks.spark.csv')\\\n  .options(header='true', inferschema='true')\\\n  .load(\"swift://DatabricksSpark.\" + name + \"/train.csv\")\ntrain.take(5)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6, Product_Category_3=14, Purchase=15200),\n Row(User_ID=1000001, Product_ID=u'P00087842', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=12, Product_Category_2=None, Product_Category_3=None, Purchase=1422),\n Row(User_ID=1000001, Product_ID=u'P00085442', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=12, Product_Category_2=14, Product_Category_3=None, Purchase=1057),\n Row(User_ID=1000002, Product_ID=u'P00285442', Gender=u'M', Age=u'55+', Occupation=16, City_Category=u'C', Stay_In_Current_City_Years=u'4+', Marital_Status=0, Product_Category_1=8, Product_Category_2=None, Product_Category_3=None, Purchase=7969)]"
                    }, 
                    "execution_count": 55
                }
            ], 
            "execution_count": 55
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "type(train)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "pyspark.sql.dataframe.DataFrame"
                    }, 
                    "execution_count": 140
                }
            ], 
            "execution_count": 140
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'cc29768790ec45439a43668592b02f84')\n    hconf.set(prefix + '.username', 'd47dac60c7684410842aa453908da4ca')\n    hconf.set(prefix + '.password', 'R1o7wzw?37&dHIMq')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name)\n\ntest = sqlContext.read.format('com.databricks.spark.csv')\\\n  .options(header='true', inferschema='true')\\\n  .load(\"swift://DatabricksSpark.\" + name + \"/test.csv\")\ntest.take(5)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[Row(User_ID=1000004, Product_ID=u'P00128942', Gender=u'M', Age=u'46-50', Occupation=7, City_Category=u'B', Stay_In_Current_City_Years=u'2', Marital_Status=1, Product_Category_1=1, Product_Category_2=11, Product_Category_3=None),\n Row(User_ID=1000009, Product_ID=u'P00113442', Gender=u'M', Age=u'26-35', Occupation=17, City_Category=u'C', Stay_In_Current_City_Years=u'0', Marital_Status=0, Product_Category_1=3, Product_Category_2=5, Product_Category_3=None),\n Row(User_ID=1000010, Product_ID=u'P00288442', Gender=u'F', Age=u'36-45', Occupation=1, City_Category=u'B', Stay_In_Current_City_Years=u'4+', Marital_Status=1, Product_Category_1=5, Product_Category_2=14, Product_Category_3=None),\n Row(User_ID=1000010, Product_ID=u'P00145342', Gender=u'F', Age=u'36-45', Occupation=1, City_Category=u'B', Stay_In_Current_City_Years=u'4+', Marital_Status=1, Product_Category_1=4, Product_Category_2=9, Product_Category_3=None),\n Row(User_ID=1000011, Product_ID=u'P00053842', Gender=u'F', Age=u'26-35', Occupation=1, City_Category=u'C', Stay_In_Current_City_Years=u'1', Marital_Status=0, Product_Category_1=4, Product_Category_2=5, Product_Category_3=12)]"
                    }, 
                    "execution_count": 16
                }
            ], 
            "execution_count": 16
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# DataFrame Manipulations"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to see datatype of columns?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.printSchema()", 
            "outputs": [
                {
                    "text": "root\n |-- User_ID: integer (nullable = true)\n |-- Product_ID: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: string (nullable = true)\n |-- Occupation: integer (nullable = true)\n |-- City_Category: string (nullable = true)\n |-- Stay_In_Current_City_Years: string (nullable = true)\n |-- Marital_Status: integer (nullable = true)\n |-- Product_Category_1: integer (nullable = true)\n |-- Product_Category_2: integer (nullable = true)\n |-- Product_Category_3: integer (nullable = true)\n |-- Purchase: integer (nullable = true)\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 142
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to Show first n observation?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "#Previewing the data set\ntrain.head(10)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6, Product_Category_3=14, Purchase=15200),\n Row(User_ID=1000001, Product_ID=u'P00087842', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=12, Product_Category_2=None, Product_Category_3=None, Purchase=1422),\n Row(User_ID=1000001, Product_ID=u'P00085442', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=12, Product_Category_2=14, Product_Category_3=None, Purchase=1057),\n Row(User_ID=1000002, Product_ID=u'P00285442', Gender=u'M', Age=u'55+', Occupation=16, City_Category=u'C', Stay_In_Current_City_Years=u'4+', Marital_Status=0, Product_Category_1=8, Product_Category_2=None, Product_Category_3=None, Purchase=7969),\n Row(User_ID=1000003, Product_ID=u'P00193542', Gender=u'M', Age=u'26-35', Occupation=15, City_Category=u'A', Stay_In_Current_City_Years=u'3', Marital_Status=0, Product_Category_1=1, Product_Category_2=2, Product_Category_3=None, Purchase=15227),\n Row(User_ID=1000004, Product_ID=u'P00184942', Gender=u'M', Age=u'46-50', Occupation=7, City_Category=u'B', Stay_In_Current_City_Years=u'2', Marital_Status=1, Product_Category_1=1, Product_Category_2=8, Product_Category_3=17, Purchase=19215),\n Row(User_ID=1000004, Product_ID=u'P00346142', Gender=u'M', Age=u'46-50', Occupation=7, City_Category=u'B', Stay_In_Current_City_Years=u'2', Marital_Status=1, Product_Category_1=1, Product_Category_2=15, Product_Category_3=None, Purchase=15854),\n Row(User_ID=1000004, Product_ID=u'P0097242', Gender=u'M', Age=u'46-50', Occupation=7, City_Category=u'B', Stay_In_Current_City_Years=u'2', Marital_Status=1, Product_Category_1=1, Product_Category_2=16, Product_Category_3=None, Purchase=15686),\n Row(User_ID=1000005, Product_ID=u'P00274942', Gender=u'M', Age=u'26-35', Occupation=20, City_Category=u'A', Stay_In_Current_City_Years=u'1', Marital_Status=1, Product_Category_1=8, Product_Category_2=None, Product_Category_3=None, Purchase=7871)]"
                    }, 
                    "execution_count": 143
                }
            ], 
            "execution_count": 143
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.show(2,truncate= True)", 
            "outputs": [
                {
                    "text": "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\nonly showing top 2 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 144
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to Count the number of rows in DataFrame?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.count(),test.count()", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "(550068, 233599)"
                    }, 
                    "execution_count": 145
                }
            ], 
            "execution_count": 145
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How many columns do we have in train and test files along with their names?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "len(train.columns), train.columns", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "(12,\n ['User_ID',\n  'Product_ID',\n  'Gender',\n  'Age',\n  'Occupation',\n  'City_Category',\n  'Stay_In_Current_City_Years',\n  'Marital_Status',\n  'Product_Category_1',\n  'Product_Category_2',\n  'Product_Category_3',\n  'Purchase'])"
                    }, 
                    "execution_count": 146
                }
            ], 
            "execution_count": 146
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "len(test.columns), test.columns", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "(11,\n ['User_ID',\n  'Product_ID',\n  'Gender',\n  'Age',\n  'Occupation',\n  'City_Category',\n  'Stay_In_Current_City_Years',\n  'Marital_Status',\n  'Product_Category_1',\n  'Product_Category_2',\n  'Product_Category_3'])"
                    }, 
                    "execution_count": 147
                }
            ], 
            "execution_count": 147
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to get the summary statistics (mean, standard deviance, min ,max , count) of numerical columns in a DataFrame?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.describe()", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "DataFrame[summary: string, User_ID: string, Occupation: string, Marital_Status: string, Product_Category_1: string, Product_Category_2: string, Product_Category_3: string, Purchase: string]"
                    }, 
                    "execution_count": 157
                }
            ], 
            "execution_count": 157
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.describe().show()", 
            "outputs": [
                {
                    "text": "+-------+------------------+-----------------+-------------------+------------------+------------------+------------------+------------------+\n|summary|           User_ID|       Occupation|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|          Purchase|\n+-------+------------------+-----------------+-------------------+------------------+------------------+------------------+------------------+\n|  count|            550068|           550068|             550068|            550068|            376430|            166821|            550068|\n|   mean|1003028.8424013031|8.076706879876669|0.40965298835780306| 5.404270017525106| 9.842329251122386|12.668243206790512| 9263.968712959126|\n| stddev|1727.5915855308265|6.522660487341778| 0.4917701263173273|3.9362113692014082| 5.086589648693526| 4.125337631575267|5023.0653938206015|\n|    min|           1000001|                0|                  0|                 1|                 2|                 3|                12|\n|    max|           1006040|               20|                  1|                20|                18|                18|             23961|\n+-------+------------------+-----------------+-------------------+------------------+------------------+------------------+------------------+\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 158
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.describe('Product_ID').show()", 
            "outputs": [
                {
                    "text": "+-------+----------+\n|summary|Product_ID|\n+-------+----------+\n|  count|    550068|\n|   mean|      null|\n| stddev|      null|\n|    min| P00000142|\n|    max|  P0099942|\n+-------+----------+\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 159
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to select column(s) from the DataFrame?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.select('User_ID','Age').show(5)", 
            "outputs": [
                {
                    "text": "+-------+----+\n|User_ID| Age|\n+-------+----+\n|1000001|0-17|\n|1000001|0-17|\n|1000001|0-17|\n|1000001|0-17|\n|1000002| 55+|\n+-------+----+\nonly showing top 5 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 160
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to find the number of distinct product in train and test files?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.select('Product_ID').distinct().count(),test.select('Product_ID').distinct().count()", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "(3631, 3491)"
                    }, 
                    "execution_count": 161
                }
            ], 
            "execution_count": 161
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "diff_cat_in_train_test=test.select('Product_ID').subtract(train.select('Product_ID'))\ndiff_cat_in_train_test.distinct().count()# For distict count", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "46"
                    }, 
                    "execution_count": 162
                }
            ], 
            "execution_count": 162
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### What if I want to calculate pair wise frequency of categorical columns?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.crosstab('Age', 'Gender').show()", 
            "outputs": [
                {
                    "text": "+----------+-----+------+\n|Age_Gender|    F|     M|\n+----------+-----+------+\n|      0-17| 5083| 10019|\n|     46-50|13199| 32502|\n|     18-25|24628| 75032|\n|     36-45|27170| 82843|\n|       55+| 5083| 16421|\n|     51-55| 9894| 28607|\n|     26-35|50752|168835|\n+----------+-----+------+\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 163
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### What If I want to get the DataFrame which won\u2019t have duplicate rows of given DataFrame?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.select('Age','Gender').dropDuplicates().show()", 
            "outputs": [
                {
                    "text": "+-----+------+\n|  Age|Gender|\n+-----+------+\n|51-55|     F|\n|51-55|     M|\n|26-35|     F|\n|26-35|     M|\n|36-45|     F|\n|36-45|     M|\n|46-50|     F|\n|46-50|     M|\n|  55+|     F|\n|  55+|     M|\n|18-25|     F|\n| 0-17|     F|\n|18-25|     M|\n| 0-17|     M|\n+-----+------+\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 164
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### What if I want to drop the all rows with null value?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.dropna().count()", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "166821"
                    }, 
                    "execution_count": 165
                }
            ], 
            "execution_count": 165
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### What if I want to fill the null values in DataFrame with constant number?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.fillna(-1).show(2)", 
            "outputs": [
                {
                    "text": "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|\n|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\nonly showing top 2 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 166
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### If I want to filter the rows in train which has Purchase more than 15000?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.filter(train.Purchase > 15000).count()", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "110523"
                    }, 
                    "execution_count": 167
                }
            ], 
            "execution_count": 167
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to find the mean of each age group in train?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.groupby('Age').agg({'Purchase': 'mean'}).show()", 
            "outputs": [
                {
                    "text": "+-----+-----------------+\n|  Age|    avg(Purchase)|\n+-----+-----------------+\n|51-55|9534.808030960236|\n|46-50|9208.625697468327|\n| 0-17|8933.464640444974|\n|36-45|9331.350694917874|\n|26-35|9252.690632869888|\n|  55+|9336.280459449405|\n|18-25|9169.663606261289|\n+-----+-----------------+\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 168
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.groupby('Age').count().show()", 
            "outputs": [
                {
                    "text": "+-----+------+\n|  Age| count|\n+-----+------+\n|51-55| 38501|\n|46-50| 45701|\n| 0-17| 15102|\n|36-45|110013|\n|26-35|219587|\n|  55+| 21504|\n|18-25| 99660|\n+-----+------+\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 169
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to create a sample DataFrame from the base DataFrame?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "t1 = train.sample(False, 0.2, 42)\nt2 = train.sample(False, 0.2, 43)\nt1.count(),t2.count()", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "(109812, 109745)"
                    }, 
                    "execution_count": 170
                }
            ], 
            "execution_count": 170
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to apply map operation on DataFrame columns?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.select('User_ID').map(lambda x:(x,1)).take(5)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[(Row(User_ID=1000001), 1),\n (Row(User_ID=1000001), 1),\n (Row(User_ID=1000001), 1),\n (Row(User_ID=1000001), 1),\n (Row(User_ID=1000002), 1)]"
                    }, 
                    "execution_count": 171
                }
            ], 
            "execution_count": 171
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# above is an rdd after applying the map, hence we used take() instead of the show()\ntype(train.select('User_ID').map(lambda x:(x,1)))", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "pyspark.rdd.PipelinedRDD"
                    }, 
                    "execution_count": 172
                }
            ], 
            "execution_count": 172
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to sort the DataFrame based on column(s)?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "type(train.orderBy(train.Purchase.desc()))", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "pyspark.sql.dataframe.DataFrame"
                    }, 
                    "execution_count": 173
                }
            ], 
            "execution_count": 173
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.orderBy(train.Purchase.desc()).show(5)", 
            "outputs": [
                {
                    "text": "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|1003160| P00052842|     M|26-35|        17|            C|                         3|             0|                10|                15|              null|   23961|\n|1002272| P00052842|     M|26-35|         0|            C|                         1|             0|                10|                15|              null|   23961|\n|1001474| P00052842|     M|26-35|         4|            A|                         2|             1|                10|                15|              null|   23961|\n|1005848| P00119342|     M|51-55|        20|            A|                         0|             1|                10|                13|              null|   23960|\n|1003045| P00052842|     M|46-50|         1|            B|                         2|             1|                10|                15|              null|   23960|\n+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\nonly showing top 5 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 174
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to add the new column in DataFrame?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.withColumn('Purchase_new', train.Purchase /2.0).select('Purchase','Purchase_new').show(5)", 
            "outputs": [
                {
                    "text": "+--------+------------+\n|Purchase|Purchase_new|\n+--------+------------+\n|    8370|      4185.0|\n|   15200|      7600.0|\n|    1422|       711.0|\n|    1057|       528.5|\n|    7969|      3984.5|\n+--------+------------+\nonly showing top 5 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 175
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to drop a column in DataFrame?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "test.drop('Comb').columns", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "['User_ID',\n 'Product_ID',\n 'Gender',\n 'Age',\n 'Occupation',\n 'City_Category',\n 'Stay_In_Current_City_Years',\n 'Marital_Status',\n 'Product_Category_1',\n 'Product_Category_2',\n 'Product_Category_3']"
                    }, 
                    "execution_count": 176
                }
            ], 
            "execution_count": 176
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### What if I want to remove some categories of Product_ID column in test that are not present in Product_ID column in train?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "diff_cat_in_train_test=test.select('Product_ID').subtract(train.select('Product_ID'))\ndiff_cat_in_train_test.distinct().count()# For distict coun", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "46"
                    }, 
                    "execution_count": 177
                }
            ], 
            "execution_count": 177
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "not_found_cat = diff_cat_in_train_test.distinct().rdd.map(lambda x: x[0]).collect()\nlen(not_found_cat)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "46"
                    }, 
                    "execution_count": 178
                }
            ], 
            "execution_count": 178
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.sql.types import StringType\nfrom pyspark.sql.functions import udf\nF1 = udf(lambda x: '-1' if x in not_found_cat else x, StringType())", 
            "outputs": [], 
            "execution_count": 179
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "k = test.withColumn(\"NEW_Product_ID\",F1(test[\"Product_ID\"])).select('NEW_Product_ID')", 
            "outputs": [], 
            "execution_count": 180
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "diff_cat_in_train_test=k.select('NEW_Product_ID').subtract(train.select('Product_ID'))\ndiff_cat_in_train_test.distinct().count()# For distinct count", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "1"
                    }, 
                    "execution_count": 181
                }
            ], 
            "execution_count": 181
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "diff_cat_in_train_test.distinct().collect()", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[Row(NEW_Product_ID=u'-1')]"
                    }, 
                    "execution_count": 182
                }
            ], 
            "execution_count": 182
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### How to Apply SQL Queries on DataFrame?"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train = sqlContext.read.format('com.databricks.spark.csv')\\\n  .options(header='true', inferschema='true')\\\n  .load(\"swift://DatabricksSpark.\" + name + \"/train.csv\")\n#train.take(5)\ntrain.show()", 
            "outputs": [
                {
                    "text": "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n|1000001| P00069042|     F| 0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n|1000001| P00248942|     F| 0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n|1000001| P00087842|     F| 0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|\n|1000001| P00085442|     F| 0-17|        10|            A|                         2|             0|                12|                14|              null|    1057|\n|1000002| P00285442|     M|  55+|        16|            C|                        4+|             0|                 8|              null|              null|    7969|\n|1000003| P00193542|     M|26-35|        15|            A|                         3|             0|                 1|                 2|              null|   15227|\n|1000004| P00184942|     M|46-50|         7|            B|                         2|             1|                 1|                 8|                17|   19215|\n|1000004| P00346142|     M|46-50|         7|            B|                         2|             1|                 1|                15|              null|   15854|\n|1000004|  P0097242|     M|46-50|         7|            B|                         2|             1|                 1|                16|              null|   15686|\n|1000005| P00274942|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    7871|\n|1000005| P00251242|     M|26-35|        20|            A|                         1|             1|                 5|                11|              null|    5254|\n|1000005| P00014542|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    3957|\n|1000005| P00031342|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    6073|\n|1000005| P00145042|     M|26-35|        20|            A|                         1|             1|                 1|                 2|                 5|   15665|\n|1000006| P00231342|     F|51-55|         9|            A|                         1|             0|                 5|                 8|                14|    5378|\n|1000006| P00190242|     F|51-55|         9|            A|                         1|             0|                 4|                 5|              null|    2079|\n|1000006|  P0096642|     F|51-55|         9|            A|                         1|             0|                 2|                 3|                 4|   13055|\n|1000006| P00058442|     F|51-55|         9|            A|                         1|             0|                 5|                14|              null|    8851|\n|1000007| P00036842|     M|36-45|         1|            B|                         1|             1|                 1|                14|                16|   11788|\n|1000008| P00249542|     M|26-35|        12|            C|                        4+|             1|                 1|                 5|                15|   19614|\n+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 6
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "type(train)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "pyspark.sql.dataframe.DataFrame"
                    }, 
                    "execution_count": 7
                }
            ], 
            "execution_count": 7
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.registerTempTable(\"train_table\")", 
            "outputs": [], 
            "execution_count": 8
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "type(\"train_table\")", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "str"
                    }, 
                    "execution_count": 10
                }
            ], 
            "execution_count": 10
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "sqlContext.sql(\"SELECT Product_ID FROM train_table\").show(5)", 
            "outputs": [
                {
                    "text": "+----------+\n|Product_ID|\n+----------+\n| P00069042|\n| P00248942|\n| P00087842|\n| P00085442|\n| P00285442|\n+----------+\nonly showing top 5 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 11
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "sqlContext.sql('select Age, max(Purchase) from train_table group by Age').show()", 
            "outputs": [
                {
                    "text": "+-----+-----+\n|  Age|  _c1|\n+-----+-----+\n|51-55|23960|\n|46-50|23960|\n| 0-17|23955|\n|36-45|23960|\n|26-35|23961|\n|  55+|23960|\n|18-25|23958|\n+-----+-----+\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 12
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Pandas vs PySpark DataFrame"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Pandas and Spark DataFrame are designed for structural and semistructral data processing. Both share some similar properties (which I have discussed above). The few differences between Pandas and PySpark DataFrame are:\n\nOperation on Pyspark DataFrame run parallel on different nodes in cluster but, in case of pandas it is not possible.\nOperations in PySpark DataFrame are lazy in nature but, in case of pandas we get the result as soon as we apply any operation.\nIn PySpark DataFrame, we can\u2019t change the DataFrame due to it\u2019s immutable property, we need to transform it. But in pandas it is not the case.\nPandas API support more operations than PySpark DataFrame. Still pandas API is more powerful than Spark.\nComplex operations in pandas are easier to perform than Pyspark DataFrame"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Transforming categorical variables to labels"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Rerun the cells, Creating the DataFrame from CSV file, to create DataFrames train and test, if required."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "train = train.fillna(-1)\ntest = test.fillna(-1)", 
            "outputs": [], 
            "execution_count": 19
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "#train.count(),test.count()", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "(550068, 233599)"
                    }, 
                    "execution_count": 21
                }
            ], 
            "execution_count": 21
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train.na.drop().count(),test.na.drop('any').count()", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "(550068, 233599)"
                    }, 
                    "execution_count": 22
                }
            ], 
            "execution_count": 22
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.feature import StringIndexer\nplan_indexer = StringIndexer(inputCol = 'Product_ID', outputCol = 'product_ID')\nlabeller = plan_indexer.fit(train)", 
            "outputs": [], 
            "execution_count": 26
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "Train1 = labeller.transform(train)\nTest1 = labeller.transform(test)", 
            "outputs": [], 
            "execution_count": 27
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "Train1.show(5)", 
            "outputs": [
                {
                    "text": "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------+\n|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|product_ID|\n+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------+\n|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|     766.0|\n|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|     183.0|\n|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|                -1|                -1|    1422|    1496.0|\n|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|                14|                -1|    1057|     481.0|\n|1000002| P00285442|     M| 55+|        16|            C|                        4+|             0|                 8|                -1|                -1|    7969|     860.0|\n+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------+\nonly showing top 5 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 28
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.feature import RFormula\nformula = RFormula(formula=\"Purchase ~ Age+ Occupation +City_Category+Stay_In_Current_City_Years+Product_Category_1+Product_Category_2+ Gender\",featuresCol=\"features\",labelCol=\"label\")", 
            "outputs": [], 
            "execution_count": 29
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "t1 = formula.fit(Train1)\ntrain1 = t1.transform(Train1)\ntest1 = t1.transform(Test1)", 
            "outputs": [], 
            "execution_count": 30
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train1.show()", 
            "outputs": [
                {
                    "text": "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------+--------------------+-------+\n|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|product_ID|            features|  label|\n+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------+--------------------+-------+\n|1000001| P00069042|     F| 0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|     766.0|(16,[6,10,13,14],...| 8370.0|\n|1000001| P00248942|     F| 0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|     183.0|(16,[6,10,13,14],...|15200.0|\n|1000001| P00087842|     F| 0-17|        10|            A|                         2|             0|                12|                -1|                -1|    1422|    1496.0|(16,[6,10,13,14],...| 1422.0|\n|1000001| P00085442|     F| 0-17|        10|            A|                         2|             0|                12|                14|                -1|    1057|     481.0|(16,[6,10,13,14],...| 1057.0|\n|1000002| P00285442|     M|  55+|        16|            C|                        4+|             0|                 8|                -1|                -1|    7969|     860.0|(16,[5,6,8,12,13,...| 7969.0|\n|1000003| P00193542|     M|26-35|        15|            A|                         3|             0|                 1|                 2|                -1|   15227|     157.0|(16,[0,6,11,13,14...|15227.0|\n|1000004| P00184942|     M|46-50|         7|            B|                         2|             1|                 1|                 8|                17|   19215|       5.0|(16,[3,6,7,10,13,...|19215.0|\n|1000004| P00346142|     M|46-50|         7|            B|                         2|             1|                 1|                15|                -1|   15854|     177.0|(16,[3,6,7,10,13,...|15854.0|\n|1000004|  P0097242|     M|46-50|         7|            B|                         2|             1|                 1|                16|                -1|   15686|      51.0|(16,[3,6,7,10,13,...|15686.0|\n|1000005| P00274942|     M|26-35|        20|            A|                         1|             1|                 8|                -1|                -1|    7871|      78.0|(16,[0,6,9,13,14,...| 7871.0|\n|1000005| P00251242|     M|26-35|        20|            A|                         1|             1|                 5|                11|                -1|    5254|      27.0|(16,[0,6,9,13,14,...| 5254.0|\n|1000005| P00014542|     M|26-35|        20|            A|                         1|             1|                 8|                -1|                -1|    3957|     128.0|(16,[0,6,9,13,14,...| 3957.0|\n|1000005| P00031342|     M|26-35|        20|            A|                         1|             1|                 8|                -1|                -1|    6073|    2318.0|(16,[0,6,9,13,14,...| 6073.0|\n|1000005| P00145042|     M|26-35|        20|            A|                         1|             1|                 1|                 2|                 5|   15665|       9.0|(16,[0,6,9,13,14,...|15665.0|\n|1000006| P00231342|     F|51-55|         9|            A|                         1|             0|                 5|                 8|                14|    5378|    1680.0|(16,[4,6,9,13,14]...| 5378.0|\n|1000006| P00190242|     F|51-55|         9|            A|                         1|             0|                 4|                 5|                -1|    2079|     927.0|(16,[4,6,9,13,14]...| 2079.0|\n|1000006|  P0096642|     F|51-55|         9|            A|                         1|             0|                 2|                 3|                 4|   13055|    1000.0|(16,[4,6,9,13,14]...|13055.0|\n|1000006| P00058442|     F|51-55|         9|            A|                         1|             0|                 5|                14|                -1|    8851|     209.0|(16,[4,6,9,13,14]...| 8851.0|\n|1000007| P00036842|     M|36-45|         1|            B|                         1|             1|                 1|                14|                16|   11788|      58.0|(16,[1,6,7,9,13,1...|11788.0|\n|1000008| P00249542|     M|26-35|        12|            C|                        4+|             1|                 1|                 5|                15|   19614|      81.0|(16,[0,6,8,12,13,...|19614.0|\n+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------+--------------------+-------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 31
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train1.select('features').show()\n", 
            "outputs": [
                {
                    "text": "+--------------------+\n|            features|\n+--------------------+\n|(16,[6,10,13,14],...|\n|(16,[6,10,13,14],...|\n|(16,[6,10,13,14],...|\n|(16,[6,10,13,14],...|\n|(16,[5,6,8,12,13,...|\n|(16,[0,6,11,13,14...|\n|(16,[3,6,7,10,13,...|\n|(16,[3,6,7,10,13,...|\n|(16,[3,6,7,10,13,...|\n|(16,[0,6,9,13,14,...|\n|(16,[0,6,9,13,14,...|\n|(16,[0,6,9,13,14,...|\n|(16,[0,6,9,13,14,...|\n|(16,[0,6,9,13,14,...|\n|(16,[4,6,9,13,14]...|\n|(16,[4,6,9,13,14]...|\n|(16,[4,6,9,13,14]...|\n|(16,[4,6,9,13,14]...|\n|(16,[1,6,7,9,13,1...|\n|(16,[0,6,8,12,13,...|\n+--------------------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 32
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "train1.select('label').show()", 
            "outputs": [
                {
                    "text": "+-------+\n|  label|\n+-------+\n| 8370.0|\n|15200.0|\n| 1422.0|\n| 1057.0|\n| 7969.0|\n|15227.0|\n|19215.0|\n|15854.0|\n|15686.0|\n| 7871.0|\n| 5254.0|\n| 3957.0|\n| 6073.0|\n|15665.0|\n| 5378.0|\n| 2079.0|\n|13055.0|\n| 8851.0|\n|11788.0|\n|19614.0|\n+-------+\nonly showing top 20 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "execution_count": 33
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Building a Machine Learning Model: Random Forest", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.regression import RandomForestRegressor\nrf = RandomForestRegressor()", 
            "outputs": [], 
            "execution_count": 39
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "(train_cv, test_cv) = train1.randomSplit([0.7, 0.3])", 
            "outputs": [], 
            "execution_count": 40
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "model1 = rf.fit(train_cv)\npredictions = model1.transform(test_cv)", 
            "outputs": [], 
            "execution_count": 41
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "from pyspark.ml.evaluation import RegressionEvaluator\nevaluator = RegressionEvaluator()\nmse = evaluator.evaluate(predictions,{evaluator.metricName:\"mse\" })\nimport numpy as np\nnp.sqrt(mse), mse", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "(3785.2924588462984, 14328438.998998655)"
                    }, 
                    "execution_count": 42
                }
            ], 
            "execution_count": 42
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "model = rf.fit(train1)\npredictions1 = model.transform(test1)", 
            "outputs": [], 
            "execution_count": 43
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "df = predictions1.selectExpr(\"User_ID as User_ID\", \"Product_ID as Product_ID\", 'prediction as Purchase')", 
            "outputs": [], 
            "execution_count": 44
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "## Did not work\n## df.toPandas().to_csv(\"swift://\"+ \"DatabricksSpark.\" + name + \"/submission1.csv\",index=False)", 
            "outputs": [], 
            "execution_count": 89
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "# this worked with Spark dataframe\n# Overwrite not working http://bit.ly/2jw5aWO\n# Container name is DatabricksSpark\n# Change file name to submissionn.csv, where n is a new number incremented number\n\nfileNameOut = \"swift://\"+ \"DatabricksSpark.\" + name + \"/submission3.csv\"\ndf.write.format('com.databricks.spark.csv').options(header='true').save(fileNameOut)", 
            "outputs": [], 
            "execution_count": 46
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'cc29768790ec45439a43668592b02f84')\n    hconf.set(prefix + '.username', 'd47dac60c7684410842aa453908da4ca')\n    hconf.set(prefix + '.password', 'R1o7wzw?37&dHIMq')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name)\n\ndf_data_1 = sqlContext.read.format('com.databricks.spark.csv')\\\n  .options(header='true', inferschema='true')\\\n  .load(\"swift://DatabricksSpark.\" + name + \"/submission3.csv\")\ndf_data_1.take(5)\n", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[Row(User_ID=1000004, Product_ID=u'P00128942', Purchase=13392.424905928661),\n Row(User_ID=1000009, Product_ID=u'P00113442', Purchase=7917.352350629922),\n Row(User_ID=1000010, Product_ID=u'P00288442', Purchase=6478.243920429045),\n Row(User_ID=1000010, Product_ID=u'P00145342', Purchase=5853.415312002871),\n Row(User_ID=1000011, Product_ID=u'P00053842', Purchase=6492.888395717076)]"
                    }, 
                    "execution_count": 48
                }
            ], 
            "execution_count": 48
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6", 
            "name": "python2", 
            "language": "python"
        }, 
        "language_info": {
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2", 
            "name": "python"
        }
    }, 
    "nbformat_minor": 0, 
    "nbformat": 4
}