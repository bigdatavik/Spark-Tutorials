{
    "nbformat_minor": 0, 
    "metadata": {
        "kernelspec": {
            "name": "python2", 
            "display_name": "Python 2 with Spark 1.6", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "2.7.11"
        }
    }, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# Comprehensive Introduction to Apache Spark, RDDs & Dataframes (using PySpark)\n\nhttps://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/\n\n## Creating a SparkContext\n\nFirst we need to create a SparkContext. We will import this from pyspark:"
        }, 
        {
            "outputs": [], 
            "source": "# from pyspark import SparkContext", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 1
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now create the SparkContext,A SparkContext represents the connection to a Spark cluster, and can be used to create an RDD and broadcast variables on that cluster.\n\n*Note! You can only have one SparkContext at a time the way we are running things here.*"
        }, 
        {
            "outputs": [], 
            "source": "# sc = SparkContext()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 4
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 6, 
                    "data": {
                        "text/plain": "<pyspark.context.SparkContext at 0x7f0020bae450>"
                    }
                }
            ], 
            "source": "# Spark context alreday exists!\nsc", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 6
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 7, 
                    "data": {
                        "text/plain": "u'1.6.0'"
                    }
                }
            ], 
            "source": "sc.version", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 7
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 18, 
                    "data": {
                        "text/plain": "[u'Think of it for a moment \\u2013 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.',\n u'',\n u'Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.',\n u'',\n u'On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.']"
                    }
                }
            ], 
            "source": "# Import any .csv file as a RDD and then reaplace with the blogtexts file name in sc.textFile\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'cc29768790ec45439a43668592b02f84')\n    hconf.set(prefix + '.username', 'd47dac60c7684410842aa453908da4ca')\n    hconf.set(prefix + '.password', 'R1o7wzw?37&dHIMq')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_19099026f8df40b6aec4353c7e897e95(name)\n\nrdd = sc.textFile(\"swift://DatabricksSpark.\" + name + \"/blogtexts\")\nrdd.take(5)\n", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 18
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## General transformations"
        }, 
        {
            "outputs": [], 
            "source": "# Transformation: map and flatMap\n\ndef Func(lines):\n      lines = lines.lower()\n      lines = lines.split()\n      return lines\nrdd1 = rdd.map(Func)\n\n# rdd1 = rdd.map(lambda lines: lines.lower()).map(lambda lines: lines.split())", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 41
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 42, 
                    "data": {
                        "text/plain": "[[u'think',\n  u'of',\n  u'it',\n  u'for',\n  u'a',\n  u'moment',\n  u'\\u2013',\n  u'1',\n  u'qunitillion',\n  u'=',\n  u'1',\n  u'million',\n  u'billion!',\n  u'can',\n  u'you',\n  u'imagine',\n  u'how',\n  u'many',\n  u'drives',\n  u'/',\n  u'cds',\n  u'/',\n  u'blue-ray',\n  u'dvds',\n  u'would',\n  u'be',\n  u'required',\n  u'to',\n  u'store',\n  u'them?',\n  u'it',\n  u'is',\n  u'difficult',\n  u'to',\n  u'imagine',\n  u'this',\n  u'scale',\n  u'of',\n  u'data',\n  u'generation',\n  u'even',\n  u'as',\n  u'a',\n  u'data',\n  u'science',\n  u'professional.',\n  u'while',\n  u'this',\n  u'pace',\n  u'of',\n  u'data',\n  u'generation',\n  u'is',\n  u'very',\n  u'exciting,',\n  u'it',\n  u'has',\n  u'created',\n  u'entirely',\n  u'new',\n  u'set',\n  u'of',\n  u'challenges',\n  u'and',\n  u'has',\n  u'forced',\n  u'us',\n  u'to',\n  u'find',\n  u'new',\n  u'ways',\n  u'to',\n  u'handle',\n  u'big',\n  u'huge',\n  u'data',\n  u'effectively.'],\n []]"
                    }
                }
            ], 
            "source": "rdd1.take(2) # It will print first 2 elements of rdd", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 42
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 43, 
                    "data": {
                        "text/plain": "[u'think', u'of', u'it', u'for', u'a']"
                    }
                }
            ], 
            "source": "rdd2 = rdd.flatMap(Func)\n\n#rdd2 = rdd.map(lambda lines: lines.lower()).flatMap(lambda lines: lines.split())\nrdd2.take(5)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 43
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 44, 
                    "data": {
                        "text/plain": "[u'think',\n u'of',\n u'it',\n u'moment',\n u'\\u2013',\n u'1',\n u'qunitillion',\n u'=',\n u'1',\n u'million']"
                    }
                }
            ], 
            "source": "# Transformation: filter\n\nstopwords = ['is','am','are','the','for','a']\nrdd3 = rdd2.filter(lambda x: x not in stopwords)\nrdd3.take(10)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 44
        }, 
        {
            "outputs": [], 
            "source": "#Transformation: groupByKey / reduceByKey ", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[(u'all', [u'all', u'allocates', u'all', u'all', u'allows', u'all', u'all', u'all', u'all', u'all', u'all', u'all'])]\n"
                }
            ], 
            "source": "rdd4 = rdd3.groupBy(lambda w: w[0:3])\nprint [(k, list(v)) for (k, v) in rdd4.take(1)]", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 48
        }, 
        {
            "outputs": [], 
            "source": "rdd3_mapped = rdd3.map(lambda x: (x,1))\nrdd3_grouped = rdd3_mapped.groupByKey()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 53
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[(u'all', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (u'elements,', [1, 1]), (u'step2:', [1]), (u'manager', [1]), (u'computation', [1, 1, 1, 1, 1, 1])]\n"
                }
            ], 
            "source": "print(list((j[0], list(j[1])) for j in rdd3_grouped.take(5)))", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 54
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 66, 
                    "data": {
                        "text/plain": "[u'manager,', u'manager,', u'manager,']"
                    }
                }
            ], 
            "source": "rdd3.filter(lambda x: x == 'manager,').collect()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 66
        }, 
        {
            "outputs": [], 
            "source": "rdd3_freq_of_words = rdd3_grouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 67
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 68, 
                    "data": {
                        "text/plain": "[(164, u'to'),\n (143, u'in'),\n (122, u'of'),\n (106, u'and'),\n (103, u'we'),\n (69, u'spark'),\n (64, u'this'),\n (63, u'data'),\n (55, u'can'),\n (52, u'apache')]"
                    }
                }
            ], 
            "source": "rdd3_freq_of_words.take(10)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 68
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 69, 
                    "data": {
                        "text/plain": "[(164, u'to'),\n (143, u'in'),\n (122, u'of'),\n (106, u'and'),\n (103, u'we'),\n (69, u'spark'),\n (64, u'this'),\n (63, u'data'),\n (55, u'can'),\n (52, u'apache')]"
                    }
                }
            ], 
            "source": "rdd3_mapped.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(10)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 69
        }, 
        {
            "outputs": [], 
            "source": "# Transformation: mapPartitions", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [], 
            "source": "def func(iterator):\n  count_spark = 0\n  count_apache = 0\n  for i in iterator:\n     if i =='spark':\n        count_spark = count_spark + 1\n     if i == 'apache':\n        count_apache = count_apache + 1\n  return (count_spark,count_apache)", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 77
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 78, 
                    "data": {
                        "text/plain": "[[49, 39], [20, 13]]"
                    }
                }
            ], 
            "source": "rdd3.mapPartitions(func).glom().collect()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 78
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 80, 
                    "data": {
                        "text/plain": "[49, 39, 20, 13]"
                    }
                }
            ], 
            "source": "rdd3.mapPartitions(func).collect()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 80
        }, 
        {
            "outputs": [], 
            "source": "# Transformation: sample", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "4768 1895\n"
                }
            ], 
            "source": "rdd3_sampled = rdd3.sample(False, 0.4, 42)\nprint len(rdd3.collect()),len(rdd3_sampled.collect())", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 81
        }, 
        {
            "outputs": [], 
            "source": "# Transformation: union", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 82
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "914 914 1828\n"
                }
            ], 
            "source": "sample1 = rdd3.sample(False,0.2,42)\nsample2 =rdd3.sample(False,0.2,42)\nunion_of_sample1_sample2 = sample1.union(sample2)\nprint len(sample1.collect()), len(sample2.collect()),len(union_of_sample1_sample2.collect())", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 83
        }, 
        {
            "outputs": [], 
            "source": "#Transformation: join", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": 84
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 85, 
                    "data": {
                        "text/plain": "[(u'operations', (1, 1)), (u'operations', (1, 1))]"
                    }
                }
            ], 
            "source": "sample1 = rdd3_mapped.sample(False,.2,42)\nsample2 = rdd3_mapped.sample(False,.2,42)\njoin_on_sample1_sample2 = sample1.join(sample2)\njoin_on_sample1_sample2.take(2)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 85
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 86, 
                    "data": {
                        "text/plain": "1485"
                    }
                }
            ], 
            "source": "# Transformation: distinct\nrdd3_distinct = rdd3.distinct()\nlen(rdd3_distinct.collect())", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 86
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 87, 
                    "data": {
                        "text/plain": "2"
                    }
                }
            ], 
            "source": "# Transformation: coalesce\nrdd3.getNumPartitions()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 87
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 88, 
                    "data": {
                        "text/plain": "1"
                    }
                }
            ], 
            "source": "rdd3_coalesce = rdd3.coalesce(1)\nrdd3_coalesce.getNumPartitions()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 88
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## General Actions"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 90, 
                    "data": {
                        "text/plain": "2"
                    }
                }
            ], 
            "source": "# Action: getNumPartitions\nrdd3.getNumPartitions()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 90
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 91, 
                    "data": {
                        "text/plain": "499500"
                    }
                }
            ], 
            "source": "# Action: Reduce\nnum_rdd = sc.parallelize(range(1,1000))\nnum_rdd.reduce(lambda x,y: x+y)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 91
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 92, 
                    "data": {
                        "text/plain": "4768"
                    }
                }
            ], 
            "source": "# Action: count\nrdd3.count()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 92
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 93, 
                    "data": {
                        "text/plain": "(999, 1, 499500, 83166.66666666667, 288.38631497813253)"
                    }
                }
            ], 
            "source": "# Action: max, min, sum, variance and stdev\nnum_rdd.max(),num_rdd.min(), num_rdd.sum(),num_rdd.variance(),num_rdd.stdev() ", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 93
        }, 
        {
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": null
        }
    ], 
    "nbformat": 4
}